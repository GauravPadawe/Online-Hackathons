{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Twitter Sentiment Analysis\n",
    "### Problem Statement\n",
    "\n",
    "- The objective of this task is to detect hate speech in tweets. For the sake of simplicity, we say a tweet contains hate speech if it has a racist or sexist sentiment associated with it. So, the task is to classify racist or sexist tweets from other tweets.\n",
    "\n",
    "- Formally, given a training sample of tweets and labels, where label '1' denotes the tweet is racist/sexist and label '0' denotes the tweet is not racist/sexist, your objective is to predict the labels on the test dataset.\n",
    "\n",
    "### Motivation\n",
    "\n",
    "- Hate  speech  is  an  unfortunately  common  occurrence  on  the  Internet.  Often social media sites like Facebook and Twitter face the problem of identifying and censoring  problematic  posts  while weighing the right to freedom of speech. The  importance  of  detecting  and  moderating hate  speech  is  evident  from  the  strong  connection between hate speech and actual hate crimes. Early identification of users promoting  hate  speech  could  enable  outreach  programs that attempt to prevent an escalation from speech to action. Sites such as Twitter and Facebook have been seeking  to  actively  combat  hate  speech. In spite of these reasons, NLP research on hate speech has been very limited, primarily due to the lack of a general definition of hate speech, an analysis of its demographic influences, and an investigation of the most effective features.\n",
    "\n",
    "### Data\n",
    "\n",
    "- Our overall collection of tweets was split in the ratio of 65:35 into training and testing data. Out of the testing data, 30% is public and the rest is private.\n",
    "\n",
    "- Data Files :\n",
    "    - train.csv - For training the models, we provide a labelled dataset of 31,962 tweets. The dataset is provided in the form of a csv file with each line storing a tweet id, its label and the tweet.\n",
    "    - There is 1 test file (public)\n",
    "    - test_tweets.csv - The test data file contains only tweet ids and the tweet text with each tweet in a new line.\n",
    " \n",
    "\n",
    "### Submission Details\n",
    "\n",
    "##### The following 3 files are to be uploaded.\n",
    "\n",
    "- test_predictions.csv - This should contain the 0/1 label for the tweets in test_tweets.csv, in the same order corresponding to the tweets in test_tweets.csv. Each 0/1 label should be in a new line.\n",
    " \n",
    "\n",
    "- A .zip file of source code - The code should produce the output file submitted and must be properly commented.\n",
    " \n",
    "\n",
    "### Evaluation Metric:\n",
    "\n",
    "- The metric used for evaluating the performance of classification model would be F1-Score.\n",
    "\n",
    "- The metric can be understood as :\n",
    "\n",
    "    - True Positives (TP) - These are the correctly predicted positive values which means that the value of actual class is yes and the value of predicted class is also yes.\n",
    "    - True Negatives (TN) - These are the correctly predicted negative values which means that the value of actual class is no and value of predicted class is also no.\n",
    "    - False Positives (FP) – When actual class is no and predicted class is yes.\n",
    "    - False Negatives (FN) – When actual class is yes but predicted class in no.\n",
    "    - Precision = TP/TP+FP\n",
    "    - Recall = TP/TP+FN \n",
    "\n",
    "- F1 Score = 2*(Recall * Precision) / (Recall + Precision)\n",
    "\n",
    "- F1 is usually more useful than accuracy, especially if for an uneven class distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "1fhTZYIQ9nPm",
    "outputId": "6dc75c06-23ba-4fd4-b04e-18ebd9cccd7f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Gaurav\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing Required packages\n",
    "#regex, numpy, pandas , tensorflow, nltk, etc\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "import warnings\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GR4fxRkweNRC"
   },
   "outputs": [],
   "source": [
    "#setting a path\n",
    "path = 'D:/Data Science/DS Prac/Datasets/PRACTICE DATASETS/Twitter Sentiment Analysis/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "N7GWdQfXBqKB",
    "outputId": "2065238a-9e82-4b9f-b57e-27d974cd82d6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label                                              tweet\n",
       "0   1      0   @user when a father is dysfunctional and is s...\n",
       "1   2      0  @user @user thanks for #lyft credit i can't us...\n",
       "2   3      0                                bihday your majesty\n",
       "3   4      0  #model   i love u take with u all the time in ...\n",
       "4   5      0             factsguide: society now    #motivation"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading training data\n",
    "train = pd.read_csv(path+'train.csv', header=0, encoding='utf-8')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "Vnnd2EP1Buhh",
    "outputId": "f95fae27-97bb-4a37-bec1-78422c7eec62"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31963</td>\n",
       "      <td>#studiolife #aislife #requires #passion #dedic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31964</td>\n",
       "      <td>@user #white #supremacists want everyone to s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>31965</td>\n",
       "      <td>safe ways to heal your #acne!!    #altwaystohe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>31966</td>\n",
       "      <td>is the hp and the cursed child book up for res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>31967</td>\n",
       "      <td>3rd #bihday to my amazing, hilarious #nephew...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                              tweet\n",
       "0  31963  #studiolife #aislife #requires #passion #dedic...\n",
       "1  31964   @user #white #supremacists want everyone to s...\n",
       "2  31965  safe ways to heal your #acne!!    #altwaystohe...\n",
       "3  31966  is the hp and the cursed child book up for res...\n",
       "4  31967    3rd #bihday to my amazing, hilarious #nephew..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reading training data\n",
    "test = pd.read_csv(path+'test.csv', header=0, encoding='utf-8')\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_sKrzA-70Csd"
   },
   "source": [
    "### Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b--IyVqpUXl8"
   },
   "outputs": [],
   "source": [
    "#eliminating unwanted characters with the help of regex\n",
    "train['tweet_clean'] = train['tweet'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)\n",
    "test['tweet_clean'] = test['tweet'].str.replace('[^\\w\\s#@/:%.,_-]', '', flags=re.UNICODE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "JyBBZDzgBxBr",
    "outputId": "4575237f-837a-4a2a-900f-c3d45fcd1433"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"stop_words = set(stopwords.words('english'))\\n#print (stop_words)\\n\\ntrain_words = np.array([w for w in train['tweet_clean'] if not w in stop_words])\\n#train_words\\n\\ntest_words = np.array([w for w in test['tweet_clean'] if not w in stop_words])\\n#test_words\\n\\ntrain_words\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop words removal\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#print (stop_words)\n",
    "\n",
    "train_words = np.array([w for w in train['tweet_clean'] if not w in stop_words])\n",
    "#train_words\n",
    "\n",
    "test_words = np.array([w for w in test['tweet_clean'] if not w in stop_words])\n",
    "#test_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "_eFPNXS-LRYP",
    "outputId": "057e8cd5-9677-4373-fdc6-bd1a4f91cc88"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run',\n",
       "       '@user @user thanks for #lyft credit i cant use cause they dont offer wheelchair vans in pdx.    #disapointed #getthanked',\n",
       "       '  bihday your majesty', ...,\n",
       "       'listening to sad songs on a monday morning otw to work is sad  ',\n",
       "       '@user #sikh #temple vandalised in in #calgary, #wso condemns  act  ',\n",
       "       'thank you @user for you follow  '], dtype='<U141')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_words = np.array([x for x in test['tweet_clean'] if not isinstance(x, int)])\n",
    "test_words\n",
    "\n",
    "train_words = np.array([x for x in train['tweet_clean'] if not isinstance(x, int)])\n",
    "train_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "g1lrHx1ZLite",
    "outputId": "430442ee-f388-450a-9bec-ff498a7d8e95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#punctuation remover\n",
    "table = str.maketrans('','', string.punctuation)\n",
    "\n",
    "stripped_train = np.array([w.translate(table) for w in train_words])\n",
    "#stripped_train = [w.translate(table) for w in train_words]\n",
    "#stripped_train = train_words.str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "#stripped_train\n",
    "\n",
    "stripped_test = np.array([w.translate(table) for w in test_words])\n",
    "#stripped_test = test['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\n",
    "stripped_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "zYGgGlMgLo98",
    "outputId": "4539c54b-2c09-4352-e94d-035f4b329cdc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new_train = []\\nnew_test = []\\n\\nshortword = re.compile(r'\\\\W*\\x08\\\\w{1,3}\\x08')\\n\\nfor i in stripped_train:\\n    line_train = shortword.sub('', i)\\n    new_train.append(line_train)\\n    \\nfor j in stripped_test:\\n    line_test = shortword.sub('', j)\\n    new_test.append(line_test)\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#short words removal\n",
    "\n",
    "'''new_train = []\n",
    "new_test = []\n",
    "\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n",
    "\n",
    "for i in stripped_train:\n",
    "    line_train = shortword.sub('', i)\n",
    "    new_train.append(line_train)\n",
    "    \n",
    "for j in stripped_test:\n",
    "    line_test = shortword.sub('', j)\n",
    "    new_test.append(line_test)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_XfS4K220pTM"
   },
   "outputs": [],
   "source": [
    "#new_train = np.array(new_train)\n",
    "#new_test = np.array(new_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "colab_type": "code",
    "id": "Tf0t8Ouff9Wz",
    "outputId": "8eec275f-f4a6-4e72-c11b-c1607fad719c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([' user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction   run',\n",
       "       'user user thanks for lyft credit i cant use cause they dont offer wheelchair vans in pdx    disapointed getthanked',\n",
       "       '  bihday your majesty', ...,\n",
       "       'listening to sad songs on a monday morning otw to work is sad  ',\n",
       "       'user sikh temple vandalised in in calgary wso condemns  act  ',\n",
       "       'thank you user for you follow  '], dtype='<U140')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stripped_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fiOOwlcohPlb"
   },
   "outputs": [],
   "source": [
    "'''def duplicate_removal(data):\n",
    "    temp = []\n",
    "    new_list =[]\n",
    "    new_str = []\n",
    "    \n",
    "    for i in data:\n",
    "        temp.append(i.split(' '))\n",
    "    \n",
    "    for j in temp:\n",
    "        mylist = list(dict.fromkeys(j))\n",
    "        new_list.append(mylist)\n",
    "    for k in new_list:\n",
    "        join_str = ' '.join(k)\n",
    "        new_str.append(join_str)\n",
    "    \n",
    "    return new_str\n",
    "    \n",
    "\n",
    "x = duplicate_removal(stripped_train)\n",
    "\n",
    "y = duplicate_removal(stripped_test)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XQ_OgjFc0JUV"
   },
   "source": [
    "### Removal of Emojis and Binary text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WtU6h_lY0dKP"
   },
   "source": [
    "- After a fair bit of Research I was able to finf out user-defined function to eliminate emojis, symbols, etc, if any. \n",
    "\n",
    "\n",
    "- Source : https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WI8IR9nJY4zf"
   },
   "outputs": [],
   "source": [
    "#emoji, symbols elimination\n",
    "\n",
    "def remove_emoji(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                u\"\\U00002702-\\U000027B0\"\n",
    "                u\"\\U000024C2-\\U0001F251\"\n",
    "                u\"\\U0001f926-\\U0001f937\"\n",
    "                u'\\U00010000-\\U0010ffff'\n",
    "                u\"\\u200d\"\n",
    "                u\"\\u2640-\\u2642\"\n",
    "                u\"\\u2600-\\u2B55\"\n",
    "                u\"\\u23cf\"\n",
    "                u\"\\u23e9\"\n",
    "                u\"\\u231a\"\n",
    "                u\"\\u3030\"\n",
    "                u\"\\ufe0f\"\"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Th-Ab6pRkdVo"
   },
   "outputs": [],
   "source": [
    "#applying function on data\n",
    "\n",
    "train_emoji_rem = []\n",
    "for i in x:\n",
    "    train_emoji_rem.append(remove_emoji(i))\n",
    "  \n",
    "  \n",
    "test_emoji_rem = []\n",
    "for i in y:\n",
    "    test_emoji_rem.append(remove_emoji(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-oZoX31UwrBo",
    "outputId": "94f73227-65fb-4835-b063-bbffcce5af83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"new_list=[]\\nfor word in emoji_rem:\\n    if word.encode('utf-8').decode('ascii','ignore') !='':\\n        new_list.append(word)\""
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''new_list=[]\n",
    "for word in emoji_rem:\n",
    "    if word.encode('utf-8').decode('ascii','ignore') !='':\n",
    "        new_list.append(word)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "rU7uGSMznfjc",
    "outputId": "76e9bd73-a3c0-4375-a86d-f891e47d271f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import unicodedata\\nfrom unidecode import unidecode\\n\\ndef deEmojify(inputString):\\n  returnString = \"\"\\n  for character in inputString:\\n    try:\\n      character.encode(\"ascii\")\\n      returnString += character\\n    except UnicodeEncodeError:\\n      returnString += \\'\\'\\n    return returnString'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import unicodedata\n",
    "from unidecode import unidecode\n",
    "\n",
    "def deEmojify(inputString):\n",
    "  returnString = \"\"\n",
    "  for character in inputString:\n",
    "    try:\n",
    "      character.encode(\"ascii\")\n",
    "      returnString += character\n",
    "    except UnicodeEncodeError:\n",
    "      returnString += ''\n",
    "    return returnString'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ijBxgP3T2hcx"
   },
   "outputs": [],
   "source": [
    "#tokenize the data and fit on data\n",
    "token = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
    "token.fit_on_texts(train_emoji_rem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EQKg4yC02oBR",
    "outputId": "8cfba15d-f5c1-4aa5-b00d-13f15c28ac3c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31962, 5000)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text to matrix using tfidf\n",
    "text_to_mat = token.texts_to_matrix(train_emoji_rem, mode='tfidf')\n",
    "text_to_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17812
    },
    "colab_type": "code",
    "id": "bcQS5OyraaLD",
    "outputId": "cb15f0fe-c10a-4beb-e83c-04bb3509d931"
   },
   "outputs": [],
   "source": [
    "#token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aLv3zfWTuNsd"
   },
   "outputs": [],
   "source": [
    "#from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Reshape, Flatten, LSTM, GRU, SimpleRNN\n",
    "\n",
    "#xtrain, xtest, ytrain, ytest = train_test_split(text_to_mat, train['label'], test_size=0.4, random_state=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "qffDGJrs1yMe",
    "outputId": "24f3db24-efeb-4fba-d7fa-8dc8bff5b321"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"LR_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\\n                                                    factor=0.1, \\n                                                    patience=10,\\n                                                    mode='auto', \\n                                                    cooldown=0,\\n                                                    min_lr=0\\n                                                   )\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#early stopping, Learning Rate optimization and model checkpoint implementation\n",
    "\n",
    "model_checkpoint = tf.keras.callbacks.ModelCheckpoint('best_mod.h5',\n",
    "                                                      monitor='val_loss',\n",
    "                                                      save_best_only=True)\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', \n",
    "                                                  patience=10\n",
    "                                                 )\n",
    "\n",
    "'''LR_reduction = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss',\n",
    "                                                    factor=0.1, \n",
    "                                                    patience=10,\n",
    "                                                    mode='auto', \n",
    "                                                    cooldown=0,\n",
    "                                                    min_lr=0\n",
    "                                                   )'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "colab_type": "code",
    "id": "_JAP_I7-2DpU",
    "outputId": "8fe5dbf0-4c85-4e3a-8a14-3d29c4e4d44a"
   },
   "outputs": [],
   "source": [
    "#Multi-layred ANN \n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(BatchNormalization(input_shape=(5000,), axis=1))\n",
    "\n",
    "#model.add(Flatten())\n",
    "\n",
    "model.add(Dense(500, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(200, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(25, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(10, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.1))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "\n",
    "#opt = tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#sgd = tf.keras.optimizers.SGD(lr=0.03, decay=1e-6, momentum=0.8, nesterov=True)\n",
    "#rms = tf.keras.optimizers.RMSprop(lr=0.3, rho=0.5, epsilon=0, decay=0.9)\n",
    "\n",
    "model.compile(optimizer='SGD', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "colab_type": "code",
    "id": "LM0Po-4V2Hzt",
    "outputId": "7b0b7f74-0bac-4557-d202-6d159e51c4e7",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 16939 samples, validate on 15023 samples\n",
      "Epoch 1/100\n",
      "16939/16939 [==============================] - 15s 899us/step - loss: 0.0285 - acc: 0.9924 - val_loss: 0.1068 - val_acc: 0.9846\n",
      "Epoch 2/100\n",
      "16939/16939 [==============================] - 11s 644us/step - loss: 0.0281 - acc: 0.9930 - val_loss: 0.1075 - val_acc: 0.9848\n",
      "Epoch 3/100\n",
      "16939/16939 [==============================] - 6s 346us/step - loss: 0.0259 - acc: 0.9924 - val_loss: 0.1127 - val_acc: 0.9844\n",
      "Epoch 4/100\n",
      "16939/16939 [==============================] - 6s 365us/step - loss: 0.0252 - acc: 0.9927 - val_loss: 0.1172 - val_acc: 0.9845\n",
      "Epoch 5/100\n",
      "16939/16939 [==============================] - 6s 347us/step - loss: 0.0229 - acc: 0.9942 - val_loss: 0.1221 - val_acc: 0.9844\n",
      "Epoch 6/100\n",
      "16939/16939 [==============================] - 6s 332us/step - loss: 0.0246 - acc: 0.9938 - val_loss: 0.1213 - val_acc: 0.9843\n",
      "Epoch 7/100\n",
      "16939/16939 [==============================] - 6s 333us/step - loss: 0.0218 - acc: 0.9943 - val_loss: 0.1215 - val_acc: 0.9844\n",
      "Epoch 8/100\n",
      "16939/16939 [==============================] - 6s 334us/step - loss: 0.0208 - acc: 0.9948 - val_loss: 0.1269 - val_acc: 0.9843\n",
      "Epoch 9/100\n",
      "16939/16939 [==============================] - 6s 338us/step - loss: 0.0188 - acc: 0.9950 - val_loss: 0.1283 - val_acc: 0.9844\n",
      "Epoch 10/100\n",
      "16939/16939 [==============================] - 6s 334us/step - loss: 0.0204 - acc: 0.9948 - val_loss: 0.1301 - val_acc: 0.9840\n",
      "Epoch 11/100\n",
      "16939/16939 [==============================] - 6s 378us/step - loss: 0.0194 - acc: 0.9953 - val_loss: 0.1310 - val_acc: 0.9837\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x27a4f544dd8>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit on data\n",
    "\n",
    "model.fit(text_to_mat, train['label'],\n",
    "          validation_split=0.47,\n",
    "          epochs=100,\n",
    "          batch_size=128,\n",
    "          shuffle=True, \n",
    "          callbacks=[model_checkpoint, early_stopping, \n",
    "                     #LR_reduction\n",
    "                    ]\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vEZ0kbyy28po",
    "outputId": "bcb63c65-9cd6-4085-a152-f7bc0b9fe58e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17197, 5000)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenizing test data\n",
    "token_test = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
    "token_test.fit_on_texts(test_emoji_rem)\n",
    "\n",
    "#text to matrix\n",
    "test_text_to_mat = token.texts_to_matrix(test_emoji_rem, mode='tfidf')\n",
    "test_text_to_mat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "lr3fbZY4CFTv",
    "outputId": "4c4d8d53-9bf7-49e1-c0a9-7ed5ad3cd52c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.2854771e-12],\n",
       "       [8.6503261e-01],\n",
       "       [2.7266386e-25],\n",
       "       ...,\n",
       "       [6.8329030e-14],\n",
       "       [3.4839701e-23],\n",
       "       [5.9806418e-16]], dtype=float32)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#prediction on test data\n",
    "\n",
    "pred = model.predict(test_text_to_mat)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "3uby2cnUCJAo",
    "outputId": "37f02a25-9f3e-4b8f-8282-de9405b12076"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.]\n",
      " [1.]\n",
      " [0.]\n",
      " ...\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "#probability rounding off\n",
    "pred[pred > 0.5] = 1\n",
    "pred[pred <= 0.5] = 0\n",
    "print (pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 227
    },
    "colab_type": "code",
    "id": "SyVQQ9G6CI4m",
    "outputId": "a738b480-bf0c-4339-8d35-5c8464579709"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31963</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31964</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31965</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31966</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31967</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       label\n",
       "id          \n",
       "31963      0\n",
       "31964      1\n",
       "31965      0\n",
       "31966      0\n",
       "31967      0"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#submission file\n",
    "final = pd.DataFrame({'id':test['id'].values, 'label':pred.flatten().astype('int')})\n",
    "final = final.set_index('id')\n",
    "final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pki-2FhPCI1U"
   },
   "outputs": [],
   "source": [
    "#wrining dataframe to csv\n",
    "final.to_csv('C:/Users/Gaurav/Desktop/sub_464.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Twitter Sent _ High F-score.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
